from typing import TYPE_CHECKING, Any

if TYPE_CHECKING:
    from lava.lava_job_submitter import LAVAJobSubmitter

from .lava_job_definition import (NUMBER_OF_ATTEMPTS_LAVA_BOOT,
                                  artifact_download_steps, generate_metadata)

# Use the same image that is being used for the hardware enablement and health-checks.
# They are pretty small (<100MB) and have all the tools we need to run LAVA, so it is a safe choice.
# You can find the Dockerfile here:
# https://gitlab.collabora.com/lava/health-check-docker/-/blob/main/Dockerfile
# And the registry here: https://gitlab.collabora.com/lava/health-check-docker/container_registry/
DOCKER_IMAGE = "registry.gitlab.collabora.com/lava/health-check-docker"


def attach_kernel_and_dtb(args, deploy_field):
    if args.kernel_image_type:
        deploy_field["kernel"]["type"] = args.kernel_image_type
    if args.dtb_filename:
        deploy_field["dtb"] = {"url": f"{args.kernel_url_prefix}/{args.dtb_filename}.dtb"}


def fastboot_deploy_actions(args: "LAVAJobSubmitter", nfsrootfs) -> list[dict[str, Any]]:
    fastboot_deploy_nfs = {
        "timeout": {"minutes": 10},
        "to": "nfs",
        "nfsrootfs": nfsrootfs,
    }

    fastboot_deploy_prepare = {
        "timeout": {"minutes": 5},
        "to": "downloads",
        "os": "oe",
        "images": {
            "kernel": {
                "url": f"{args.kernel_url_prefix}/{args.kernel_image_name}",
            },
        },
        "postprocess": {
            "docker": {
                "image": DOCKER_IMAGE,
                "steps": [
                    f"cat Image.gz {args.dtb_filename}.dtb > Image.gz+dtb",
                    "mkbootimg --kernel Image.gz+dtb"
                    + ' --cmdline "root=/dev/nfs rw nfsroot=$NFS_SERVER_IP:$NFS_ROOTFS,tcp,hard rootwait ip=dhcp init=/init"'
                    + " --pagesize 4096 --base 0x80000000 -o boot.img",
                ],
            }
        },
    }

    fastboot_deploy = {
        "timeout": {"minutes": 2},
        "to": "fastboot",
        "docker": {
            "image": DOCKER_IMAGE,
        },
        "images": {
            "boot": {"url": "downloads://boot.img"},
        },
    }

    # URLs to our kernel rootfs to boot from, both generated by the base
    # container build
    attach_kernel_and_dtb(args, fastboot_deploy_prepare)

    return [{"deploy": d} for d in (fastboot_deploy_nfs, fastboot_deploy_prepare, fastboot_deploy)]


def tftp_deploy_actions(args: "LAVAJobSubmitter", nfsrootfs) -> list[dict[str, Any]]:
    tftp_deploy = {
        "timeout": {"minutes": 5},
        "to": "tftp",
        "os": "oe",
        "kernel": {
            "url": f"{args.kernel_url_prefix}/{args.kernel_image_name}",
        },
        "nfsrootfs": nfsrootfs,
    }
    attach_kernel_and_dtb(args, tftp_deploy)

    return [{"deploy": d} for d in [tftp_deploy]]


def init_stage1_steps(args: "LAVAJobSubmitter") -> list[str]:
    run_steps = []
    # job execution script:
    #   - inline .gitlab-ci/common/init-stage1.sh
    #   - fetch and unpack per-pipeline build artifacts from build job
    #   - fetch and unpack per-job environment from lava-submit.sh
    #   - exec .gitlab-ci/common/init-stage2.sh

    with open(args.first_stage_init, "r") as init_sh:
        run_steps += [x.rstrip() for x in init_sh if not x.startswith("#") and x.rstrip()]
    # We cannot distribute the Adreno 660 shader firmware inside rootfs,
    # since the license isn't bundled inside the repository
    if args.device_type == "sm8350-hdk":
        run_steps.append(
            "curl -L --retry 4 -f --retry-all-errors --retry-delay 60 "
            + "https://github.com/allahjasif1990/hdk888-firmware/raw/main/a660_zap.mbn "
            + '-o "/lib/firmware/qcom/sm8350/a660_zap.mbn"'
        )

    return run_steps


def test_actions(args: "LAVAJobSubmitter") -> list[dict[str, Any]]:
    # skeleton test definition: only declaring each job as a single 'test'
    # since LAVA's test parsing is not useful to us
    run_steps = []
    test = {
        "timeout": {"minutes": args.job_timeout_min},
        "failure_retry": 1,
        "definitions": [
            {
                "name": "mesa",
                "from": "inline",
                "lava-signal": "kmsg",
                "path": "inline/mesa.yaml",
                "repository": {
                    "metadata": {
                        "name": "mesa",
                        "description": "Mesa test plan",
                        "os": ["oe"],
                        "scope": ["functional"],
                        "format": "Lava-Test Test Definition 1.0",
                    },
                    "run": {"steps": run_steps},
                },
            }
        ],
    }

    run_steps += init_stage1_steps(args)
    run_steps += artifact_download_steps(args)

    run_steps += [
        f"mkdir -p {args.ci_project_dir}",
        f"curl {args.build_url} | tar --zstd -x -C {args.ci_project_dir}",
        # Sleep a bit to give time for bash to dump shell xtrace messages into
        # console which may cause interleaving with LAVA_SIGNAL_STARTTC in some
        # devices like a618.
        "sleep 1",
        # Putting CI_JOB name as the testcase name, it may help LAVA farm
        # maintainers with monitoring
        f"lava-test-case '{args.project_name}_{args.mesa_job_name}' --shell /init-stage2.sh",
    ]

    return [{"test": t} for t in [test]]


def tftp_boot_action(args: "LAVAJobSubmitter") -> dict[str, Any]:
    tftp_boot = {
        "failure_retry": NUMBER_OF_ATTEMPTS_LAVA_BOOT,
        "method": args.boot_method,
        "prompts": ["lava-shell:"],
        "commands": "nfs",
    }

    return tftp_boot


def fastboot_boot_action(args: "LAVAJobSubmitter") -> dict[str, Any]:
    fastboot_boot = {
        "timeout": {"minutes": 2},
        "docker": {"image": DOCKER_IMAGE},
        "failure_retry": NUMBER_OF_ATTEMPTS_LAVA_BOOT,
        "method": args.boot_method,
        "prompts": ["lava-shell:"],
        "commands": ["set_active a"],
    }

    return fastboot_boot


def generate_lava_yaml_payload(args: "LAVAJobSubmitter") -> dict[str, Any]:
    """
    Generates a YAML payload for submitting a LAVA job, based on the provided arguments.

    Args:
      args ("LAVAJobSubmitter"): The `args` parameter is an instance of the `LAVAJobSubmitter`
        class. It contains various properties and methods that are used to configure and submit a
        LAVA job.

    Returns:
        a dictionary containing the values generated by the `generate_metadata` function and the
        actions for the LAVA job submission.
    """
    values = generate_metadata(args)
    nfsrootfs = {
        "url": f"{args.rootfs_url_prefix}/lava-rootfs.tar.zst",
        "compression": "zstd",
    }

    if args.boot_method == "fastboot":
        values["actions"] = [
            *fastboot_deploy_actions(args, nfsrootfs),
            {"boot": fastboot_boot_action(args)},
        ]
    else:  # tftp
        values["actions"] = [
            *tftp_deploy_actions(args, nfsrootfs),
            {"boot": tftp_boot_action(args)},
        ]

    values["actions"].extend(test_actions(args))

    return values
